{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Рубежный контроль №2\n",
    "\n",
    "Тема: Методы обработки текстов.\n",
    "\n",
    "Необходимо решить задачу классификации текстов на основе любого выбранного Вами датасета (кроме примера, который рассматривался в лекции). Классификация может быть бинарной или многоклассовой. Целевой признак из выбранного Вами датасета может иметь любой физический смысл, примером является задача анализа тональности текста.\n",
    "\n",
    "Необходимо сформировать два варианта векторизации признаков - на основе CountVectorizer и на основе TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "from scipy import stats\n",
    "from IPython.display import Image\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC, OneClassSVM, SVR, NuSVR, LinearSVR\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка и первичный анализ данных\n",
    "\n",
    "Используем данные из соревнования [Sentiment140.](http://help.sentiment140.com/for-students/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных sentiment140\n",
    "sentiment140_df = pd.read_csv(\"trainingandtestdata/testdata.manual.2009.06.14.csv\", delimiter=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a CSV with emoticons removed. Data file format has 6 fields:\n",
    "- 0 - the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "- 1 - the id of the tweet (2087)\n",
    "- 2 - the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "- 3 - the query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "- 4 - the user that tweeted (robotickilldozr)\n",
    "- 5 - the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1                             2        3         4  \\\n",
       "0  4  3  Mon May 11 03:17:40 UTC 2009  kindle2    tpryan   \n",
       "1  4  4  Mon May 11 03:18:03 UTC 2009  kindle2    vcu451   \n",
       "2  4  5  Mon May 11 03:18:54 UTC 2009  kindle2    chadfu   \n",
       "3  4  6  Mon May 11 03:19:04 UTC 2009  kindle2     SIX15   \n",
       "4  4  7  Mon May 11 03:21:41 UTC 2009  kindle2  yamarama   \n",
       "\n",
       "                                                   5  \n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3  @kenburbary You'll love your Kindle2. I've had...  \n",
       "4  @mikefish  Fair enough. But i have the Kindle2...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment140_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>I loooooooovvvvvveee my Kindle2. Not that the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ok, first assesment of the kindle2 ...it fucki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>You'll love your Kindle2. I've had mine for a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Fair enough. But i have the Kindle2 and I thi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   value                                               text\n",
       "0      4  I loooooooovvvvvveee my Kindle2. Not that the ...\n",
       "1      4  Reading my kindle2...  Love it... Lee childs i...\n",
       "2      4  Ok, first assesment of the kindle2 ...it fucki...\n",
       "3      4  You'll love your Kindle2. I've had mine for a ...\n",
       "4      4   Fair enough. But i have the Kindle2 and I thi..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment140_df = pd.DataFrame(sentiment140_df,columns=[0,5])\n",
    "sentiment140_df.columns = ['value','text']\n",
    "# Удалить строку ’@id‘\n",
    "sentiment140_df['text'] = sentiment140_df['text'].str.replace(r\"\\@[a-zA-Z0-9_]{1,}\\s\",'',regex=True)\n",
    "# Удалить строку ‘#’\n",
    "sentiment140_df['text'] = sentiment140_df['text'].str.replace('#','',regex=False)\n",
    "sentiment140_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment140_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reading my kindle2...  Love it... Lee childs is good read.',\n",
       " 'Ok, first assesment of the kindle2 ...it fucking rocks!!!',\n",
       " \"You'll love your Kindle2. I've had mine for a few months and never looked back. The new big one is huge! No need for remorse! :)\",\n",
       " \" Fair enough. But i have the Kindle2 and I think it's perfect  :)\",\n",
       " \"no. it is too big. I'm quite happy with the Kindle2.\",\n",
       " 'Fuck this economy. I hate aig and their non loan given asses.',\n",
       " 'Jquery is my new best friend.',\n",
       " 'Loves twitter',\n",
       " 'how can you not love Obama? he makes jokes about himself.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сформируем общий словарь для обучения моделей из обучающей и тестовой выборки\n",
    "vocab_list = sentiment140_df['text'].tolist()\n",
    "vocab_list[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков - 2174\n"
     ]
    }
   ],
   "source": [
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(vocab_list)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my=1283\n",
      "kindle2=1077\n",
      "not=1339\n",
      "that=1879\n",
      "the=1880\n",
      "dx=631\n",
      "is=1029\n",
      "cool=490\n",
      "but=351\n"
     ]
    }
   ],
   "source": [
    "for i in list(corpusVocab)[1:10]:\n",
    "    print('{}={}'.format(i, corpusVocab[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация текста на основе модели \"мешка слов\"\n",
    "\n",
    "Векторизация текста поддерживается библиотекой [scikit-learn.](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "### Использование класса [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) \n",
    "\n",
    "Подсчитывает количество слов словаря, входящих в данный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = vocabVect.transform(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<498x2174 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6324 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2174"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Размер нулевой строки\n",
    "len(test_features.todense()[0].getA1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Непустые значения нулевой строки\n",
    "[i for i in test_features.todense()[0].getA1() if i>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['addictive', 'adidas', 'administration', 'admissions', 'adobe',\n",
       "       'ads', 'advance', 'advice', 'aerospace', 'africa', 'after',\n",
       "       'afternoon', 'again', 'agency', 'ago', 'ah', 'aha', 'ahead', 'ahh',\n",
       "       'ahhh'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect.get_feature_names_out()[100:120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование класса [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) \n",
    "\n",
    "Вычисляет специфичность текста в корпусе текстов на основе метрики [TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<498x12569 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18019 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfv = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidf_ngram_features = tfidfv.fit_transform(vocab_list)\n",
    "tfidf_ngram_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_ngram_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12569"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Размер нулевой строки\n",
    "len(tfidf_ngram_features.todense()[0].getA1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0995053061454312,\n",
       " 0.1436959778225222,\n",
       " 0.16079126321438478,\n",
       " 0.1381925251647641,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.1436959778225222,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.07817162915475566,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.12574165668347914,\n",
       " 0.1436959778225222,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.11660059153729378,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.10279862873219407,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.07418502268287902,\n",
       " 0.12660069243065958,\n",
       " 0.16079126321438478,\n",
       " 0.09753119015737924,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.15079116232101897,\n",
       " 0.16079126321438478,\n",
       " 0.13369587692915638,\n",
       " 0.09400185348767312,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.09989716414758186,\n",
       " 0.15079116232101897,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478,\n",
       " 0.16079126321438478]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Непустые значения нулевой строки\n",
    "[i for i in tfidf_ngram_features.todense()[0].getA1() if i>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение задачи анализа тональности текста на основе модели \"мешка слов\"\n",
    "\n",
    "С использованием кросс-валидации попробуем применить к корпусу текстов различные варианты векторизации и классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
    "    for v in vectorizers_list:\n",
    "        for c in classifiers_list:\n",
    "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
    "            score = cross_val_score(pipeline1, sentiment140_df['text'], sentiment140_df['value'], scoring='accuracy', cv=3).mean()\n",
    "            print('Векторизация - {}'.format(v))\n",
    "            print('Модель для классификации - {}'.format(c))\n",
    "            print('Accuracy = {}'.format(score))\n",
    "            print('===========================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификаторы\n",
    "|  Классификатор №1  | Классификатор №2  |\n",
    "|  ----  | ----  |\n",
    "|LogisticRegression | Multinomial Naive Bayes - MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '04fo': 2, '10': 3, '100': 4,\n",
      "                            '1000': 5, '12': 6, '13t7nr': 7, '15mp': 8, '16': 9,\n",
      "                            '16209': 10, '16szl1': 11, '17': 12, '1796': 13,\n",
      "                            '1988': 14, '19epah': 15, '19j2d': 16, '1aikhf': 17,\n",
      "                            '1st': 18, '1zlff': 19, '200': 20, '2009': 21,\n",
      "                            '2010': 22, '2012': 23, '22': 24, '24': 25,\n",
      "                            '299': 26, '2boqu': 27, '2cuig': 28, '2day': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0)\n",
      "Accuracy = 0.6506024096385542\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '04fo': 2, '10': 3, '100': 4,\n",
      "                            '1000': 5, '12': 6, '13t7nr': 7, '15mp': 8, '16': 9,\n",
      "                            '16209': 10, '16szl1': 11, '17': 12, '1796': 13,\n",
      "                            '1988': 14, '19epah': 15, '19j2d': 16, '1aikhf': 17,\n",
      "                            '1st': 18, '1zlff': 19, '200': 20, '2009': 21,\n",
      "                            '2010': 22, '2012': 23, '22': 24, '24': 25,\n",
      "                            '299': 26, '2boqu': 27, '2cuig': 28, '2day': 29, ...})\n",
      "Модель для классификации - MultinomialNB(alpha=0.5)\n",
      "Accuracy = 0.6867469879518072\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'00': 0, '000': 1, '04fo': 2, '10': 3, '100': 4,\n",
      "                            '1000': 5, '12': 6, '13t7nr': 7, '15mp': 8, '16': 9,\n",
      "                            '16209': 10, '16szl1': 11, '17': 12, '1796': 13,\n",
      "                            '1988': 14, '19epah': 15, '19j2d': 16, '1aikhf': 17,\n",
      "                            '1st': 18, '1zlff': 19, '200': 20, '2009': 21,\n",
      "                            '2010': 22, '2012': 23, '22': 24, '24': 25,\n",
      "                            '299': 26, '2boqu': 27, '2cuig': 28, '2day': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0)\n",
      "Accuracy = 0.676706827309237\n",
      "===========================\n",
      "Векторизация - TfidfVectorizer(vocabulary={'00': 0, '000': 1, '04fo': 2, '10': 3, '100': 4,\n",
      "                            '1000': 5, '12': 6, '13t7nr': 7, '15mp': 8, '16': 9,\n",
      "                            '16209': 10, '16szl1': 11, '17': 12, '1796': 13,\n",
      "                            '1988': 14, '19epah': 15, '19j2d': 16, '1aikhf': 17,\n",
      "                            '1st': 18, '1zlff': 19, '200': 20, '2009': 21,\n",
      "                            '2010': 22, '2012': 23, '22': 24, '24': 25,\n",
      "                            '299': 26, '2boqu': 27, '2cuig': 28, '2day': 29, ...})\n",
      "Модель для классификации - MultinomialNB(alpha=0.5)\n",
      "Accuracy = 0.646586345381526\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab), TfidfVectorizer(vocabulary = corpusVocab)]\n",
    "#classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
    "classifiers_list = [LogisticRegression(C=3.0), MultinomialNB(alpha=0.5)]\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделим выборку на обучающую и тестовую и проверим решение для лучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sentiment140_df['text'], sentiment140_df['value'], test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.8625\n",
      "2 \t 0.6911764705882353\n",
      "4 \t 0.5346534653465347\n"
     ]
    }
   ],
   "source": [
    "sentiment(TfidfVectorizer(), LogisticRegression(C=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.75\n",
      "2 \t 0.6617647058823529\n",
      "4 \t 0.5643564356435643\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), LogisticRegression(C=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.8875\n",
      "2 \t 0.45588235294117646\n",
      "4 \t 0.5445544554455446\n"
     ]
    }
   ],
   "source": [
    "sentiment(TfidfVectorizer(), MultinomialNB(alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.8\n",
      "2 \t 0.5294117647058824\n",
      "4 \t 0.594059405940594\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), MultinomialNB(alpha=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9\n",
      "2 \t 0.38235294117647056\n",
      "4 \t 0.44554455445544555\n"
     ]
    }
   ],
   "source": [
    "sentiment(TfidfVectorizer(), MultinomialNB(alpha=0.85))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.85\n",
      "2 \t 0.5294117647058824\n",
      "4 \t 0.5247524752475248\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), MultinomialNB(alpha=0.85))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a828023ae46d8d6a11b0f5826fd5b558bb64cc1daab6c152b4e815b5cd93ecd8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
